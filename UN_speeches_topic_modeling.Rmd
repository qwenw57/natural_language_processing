---
title: "ECON630 Lab5 - Topic modeling"
output: html_document
---
#### Qingwen Wang


```{r message=FALSE, warning=FALSE}
source("~/Dropbox/ECON630/functions/load_NLP_env.R")
load_NLP_env("~/Dropbox/ECON630/functions/")
```

```{r}
speeches <- read.csv("~/Dropbox/ECON630/datasets/UN_speeches.csv")
speeches_preprocessed <- pre_process_corpus(speeches, "text", root_gen = "lemmatize", extra_stopwords = c('united states','england', 'france', 'italy', 'germany', 'japan', 'canada','russia'))
```

Data preprocessing:
```{r}
# build DTM and convert it to a sparse corpus
it <- itoken(speeches_preprocessed, tokenizer = word_tokenizer)
vocab_full <- create_vocabulary(it, ngram = c(1,3))

lbound <- 10
ubound <- 362
vocab <- vocab_full[vocab_full$doc_count > lbound & vocab_full$doc_count < ubound,]
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it,vectorizer)
sparse_corpus <- Matrix(dtm, sparse = T)
```

Find optimal k -- number of topics
```{r eval=FALSE}
# convert matrix into document-level list of vocab counts
docs <- apply(dtm, 1, function(x){
  tmp <- as.data.frame(x)
  tmp$vocab <- 1:nrow(tmp)
  tmp <- tmp[tmp[,1] >0,]
  tmp <- as.matrix.data.frame(t(tmp[, c(2,1)]))
  return(tmp)
})

# run ksearch function and analyze results
ksearch <- searchK(documents = docs, vocab = colnames(dtm), K = c(3:21), init.type = 'LDA')
```
```{r eval=FALSE, include=FALSE}
saveRDS(ksearch, file = "~/Dropbox/ECON630/assignment/ksearch_result.rds")
```

```{r include=FALSE}
ksearch <- readRDS("~/Dropbox/ECON630/assignment/ksearch_result.rds")
```


```{r}
data <- data.frame(ksearch$results[,1:3])
ggplot(data, mapping = aes( x = as.numeric(K),  y = as.numeric(exclus)))+ geom_point() +geom_line()
```
```{r}
plot(ksearch)
```


Semantic coherence is maximized when the most probable words in a given topic frequently co-occur together, and itâ€™s a metric that correlates well with human judgment of topic quality. Having high semantic coherence is relatively easy, though, there is only a few topics dominated by very common words. The held-out likelihood is highest around 12, and the residuals are lowest around 13, so perhaps a good number of topics would be around there.Semantic coherence when k is 12 is better than that when k is 13. based overall consideration, i think k = 12 could be a good one.

```{r echo = T, results = 'hide'}
topic_model <- stm(sparse_corpus, init.type = 'LDA', seed = 12345, K = 12)
```
```{r}
topic_content <- as.data.frame(t(exp(topic_model$beta$logbeta[[1]])))
apply(topic_content, 2, function(x) {topic_model$vocab[order(x, decreasing = T)[1:10]]})
```
```{r}
topic_prevalence <- as.data.frame(topic_model$theta)
topic_names <- apply(topic_content, 2, function(x) {paste(topic_model$vocab[order(x,
                                      decreasing = T)[1:6]], collapse = " ")})
mean(apply(topic_prevalence, 1, max))
```


```{r}
topic_names <- apply(topic_content, 2, function(x) {paste(topic_model$vocab[order(x,
                                      decreasing = T)[1:6]], collapse = " ")})
topic_names
```

```{r }
df <- topic_prevalence
colnames(df) <- topic_names
df$year <- as.character(speeches$year)
df <- melt(df, id.vars = 'year', value.name = 'proportion', variable.name = 'topic')

ggplot(df, aes(x = topic, y = proportion, fill = topic)) + geom_bar(stat = 'identity') +
  theme(axis.text.x = element_blank(), axis.text.y = element_blank(), legend.position = "none") +  
  coord_flip() + facet_wrap(~ year, ncol = 9)

```


Our mean prevalence is 0.5330991, which means we covers a lots of different topics with different context in this model. Main topics includes soviet government, human right, reformation,european affairs, nuclear and terrorism.  <br>
for years before 1991, G8 countries seemed to cover lots of topics on the conference more equally, and the most hottest topic is about soviet government. For years after 1991, we can see they seem to align a focus theme on each year UN conference. 1992~1996 about peace, development and reformation; 2001~2007 is mostly talking about terrorism; 2008~2016 is about both terrorism, war, and demoracy. <br>


```{r}
library(pals)
ggplot(df, aes(x = year, y = proportion, fill = topic)) + geom_bar(stat = 'identity') +
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "topic") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position="bottom", legend.text=element_text(size=6)) +
  guides(fill = guide_legend(title.position = 'top', ncol = 2))
```

```{r fig.height=10, fig.width=5}
ggplot(data = df, aes(x = year, y = proportion, color = topic))+
  geom_line()+ 
  facet_wrap(~topic, ncol = 2)+
  theme(legend.position = 'bottom', axis.text.x = element_text(angle = 45, hjust = 1), legend.text=element_text(size=6)) +

  scale_x_discrete(breaks=seq(1970, 2016, 4))+
    guides(fill = guide_legend(title.position = 'top', ncol = 2))
```


```{r}
df$decade <- paste0(substr(df$year, 0, 3), "0")
df_10 <- df %>% group_by(decade, topic) %>% summarise(mean(proportion))
colnames(df_10) <- c("decade","topic","proportion")

ggplot(df_10, aes(x=decade,y = proportion, color = topic)) + 
  geom_line(group = df_10$topic)+
    theme(legend.position = 'bottom', axis.text.x = element_text(angle = 90, hjust = 1), legend.text=element_text(size=4))+
    guides(fill = guide_legend(title.position = 'top', ncol = 2))

```

From the above stacked bar, topic distribution, and line trend, we can obverse: <br> 
Topic about soviet government is prevalent around 1983, but rarely discussed anymore after 1991. <br>
Topic about development and reform is prevalent from 1990 to 2000. <br>
Topic about terrorism is mainly talked a lot after 2001. <br>
Topic about human right is always on agenda across the past 40 years. <br>
the most important topic in 1970s is about government organization;  in 1980s is about soviet government ; in 1990s is about peaceful development and reformation; in 2000s is about terrorism; in 2010s is about democracy.